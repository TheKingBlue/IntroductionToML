{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlGsOpLHUsje"
   },
   "source": [
    "# Practical 1: Dimensionality Reduction\n",
    "\n",
    "**Course:** WBCS032-05 Introduction to Machine Learning  \n",
    "**Student Names:**  Winand Metz, Matthias Nijman  \n",
    "**Student Numbers:**  S6417221, S4667999\n",
    "\n",
    "---\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "In this assignment, you will implement Principal Component Analysis (PCA) to reduce the dimensionality of the data, as discussed in the lecture.\n",
    "\n",
    "You will work with the `COIL20.mat` dataset on Themis. The dataset consists of 1440 images, where each image has a size of $32 \\times 32$ pixels and is flattened into a vector of length 1024. All images are stored in one matrix of size $1440 \\times 1024$, where each row represents one image and each column corresponds to a pixel. The images come from 20 different objects, and each object is recorded at 72 different rotation angles, with a rotation step of 5 degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIn3pIlGVPCt"
   },
   "source": [
    "## 1. Introduction (1 point)\n",
    "\n",
    "Describe the goal of this assignment and briefly explain why dimensionality reduction and PCA are useful in data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdmLSgCTVn-_"
   },
   "source": [
    "**Your answer here:**\n",
    "\n",
    "The goal of this assignment is to apply and experiment with PCA to develop an understanding of dimensionality reduction through basic feature extraction. A secondary goal of the assignment is that it serves as a first introduction to foundational mathematical concepts widely used in machine learning. PCA itself is a preprocessing technique that compresses the data by reducing the dimensionality through encoding information in variance in lower dimensions, which decreases storage and computation needs during analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wotxOyDkVopK"
   },
   "source": [
    "## 2. Methods (3 points)\n",
    "\n",
    "### 2.1 Explain the PCA Algorithm (0.5 points)\n",
    "Explain the PCA algorithm in a general manner.\n",
    "\n",
    "**Your answer here:**  \n",
    "PCA (Principal Component Analysis) is a linear method of feature extraction for unsupervised machine learning.\n",
    "The goal is to find a new set of dimensions that are the combinations of the original dimensions.\n",
    "This is done by mapping data points from a high-dimensional space to a low-dimensional space while minimizing information loss.\n",
    "The steps of PCA are:\n",
    "1. Standardize the data set.\n",
    "2. Compute the principal components:\n",
    "    - Compute the co-variance matrix.\n",
    "    - Compute eigenvalues and eigenvectors.\n",
    "3. Choose reduced dimensionality *d*.\n",
    "4. Only pick the first *d* eigenvectors, where the eigenvectors are the principal components.  \n",
    "In general, the i-th eigenvector of the covariance matrix is associated to the i-th eigenvalue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implementation (2.5 points)\n",
    "\n",
    "You need to implement the PCA algorithm **yourself**. Both the code quality and correctness will be graded.\n",
    "\n",
    "*__Note:__* **Do not change the cell labels! Themis will use them to automatically grade your submission.**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 47,
>>>>>>> 657b6be3e1552cdf6f09d1513e969283a5323daa
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T10:40:04.949222200Z",
     "start_time": "2026-02-11T10:40:04.348029600Z"
    },
    "tags": [
     "vars"
    ]
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Data configuration\n",
    "data_file_path = 'COIL20.mat'\n",
    "image_shape = (32, 32)\n",
    "\n",
    "# PCA parameters\n",
    "d = 40  # Target dimensionality\n",
    "\n",
    "# t-SNE parameters\n",
    "tsne_perplexity = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Algorithm Steps\n",
    "\n",
    "Implement the following steps:\n",
    "\n",
    "1. **Normalize the data:**\n",
    "   $$Z = \\frac{X - \\mu}{\\sigma}$$\n",
    "   where $\\mu$ is the mean of all samples and $\\sigma$ is the standard deviation.\n",
    "\n",
    "2. **Compute the covariance matrix of the normalized data** and obtain its eigenvalues $D$ and eigenvectors $U$.  \n",
    "   You may use `np.linalg.eig` in Python.\n",
    "\n",
    "3. **Sort the eigenvectors in descending order of their eigenvalues** and select the first $d$ principal components to form $U_d$.\n",
    "\n",
    "4. **Reduce the dimensionality of the data** by projecting the normalized data onto the selected principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T10:29:54.224152600Z",
     "start_time": "2026-02-11T10:29:54.218506500Z"
    },
    "tags": [
     "pca"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement PCA here\n",
    "def normalize_data(x):\n",
    "    \"\"\"\n",
    "    Normalize data for PCA.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Dataset matrix (each column represents a variable)\n",
    "\n",
    "    Returns:\n",
    "        z (np.ndarray): Normalized dataset matrix (each column represents a variable)\n",
    "    \"\"\"\n",
    "    z = np.empty((x.shape))\n",
    "    mean = x.mean()\n",
    "    std = x.std()\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i, j] = (x[i, j] - mean) / std\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "def compute_covariance_matrix():\n",
    "    \"\"\"\n",
    "    Computes the covariance matrix of normalized data.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "def sort_eigenvectors():\n",
    "    \"\"\"\n",
    "    Sorts the eigenvectors in descending order of their eigenvalues.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "def reduce_dimensionality():\n",
    "    \"\"\"\n",
    "    Reduces the dimensionality of the data.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "def PCA(x, d):\n",
    "    \"\"\"\n",
    "    Apply Principal Component Analysis.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Dataset matrix (each column represents a variable)\n",
    "        d (int): Dimensionality of the projection\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U_d, eigen_values, Z_d)\n",
    "            - U_d (np.ndarray): Matrix of principal components, sorted descending\n",
    "            - eigen_values (np.ndarray): Eigenvalues, sorted descending\n",
    "            - Z_d (np.ndarray): Reduced version of the dataset\n",
    "    \"\"\"\n",
    "    z = normalize_data(x)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 49,
>>>>>>> 657b6be3e1552cdf6f09d1513e969283a5323daa
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T10:29:54.232588Z",
     "start_time": "2026-02-11T10:29:54.224152600Z"
    },
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01568627, 0.01568627, 0.01568627, ..., 0.01568627, 0.01568627,\n",
       "        0.01568627],\n",
       "       [0.01960784, 0.01960784, 0.01960784, ..., 0.01960784, 0.01960784,\n",
       "        0.01960784],\n",
       "       [0.01568627, 0.01568627, 0.01568627, ..., 0.01568627, 0.01568627,\n",
       "        0.01568627],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]], shape=(1440, 1024))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract dataset \n",
    "data_dict = scipy.io.loadmat(data_file_path)\n",
    "x = data_dict['X']\n",
    "# {x_1 -> x_1440}, where x_i has n (dimensions) of 1024"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply PCA to the dataset\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m U_d, eigen_values, Z_d \u001b[38;5;241m=\u001b[39m PCA(x,d)\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
=======
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.90720359, -0.90720359, -0.90720359, ..., -0.90720359,\n",
       "        -0.90720359, -0.90720359],\n",
       "       [-0.89475854, -0.89475854, -0.89475854, ..., -0.89475854,\n",
       "        -0.89475854, -0.89475854],\n",
       "       [-0.90720359, -0.90720359, -0.90720359, ..., -0.90720359,\n",
       "        -0.90720359, -0.90720359],\n",
       "       ...,\n",
       "       [-0.95698382, -0.95698382, -0.95698382, ..., -0.95698382,\n",
       "        -0.95698382, -0.95698382],\n",
       "       [-0.95698382, -0.95698382, -0.95698382, ..., -0.95698382,\n",
       "        -0.95698382, -0.95698382],\n",
       "       [-0.95698382, -0.95698382, -0.95698382, ..., -0.95698382,\n",
       "        -0.95698382, -0.95698382]], shape=(1440, 1024))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 657b6be3e1552cdf6f09d1513e969283a5323daa
    }
   ],
   "source": [
    "# Apply PCA to the dataset\n",
    "#U_d, eigen_values, Z_d = PCA(x,d)\n",
    "PCA(x,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimental Results (4 points)\n",
    "\n",
    "*__Note:__* This section is not graded by Themis.\n",
    "\n",
    "### 3.1 Eigenvalue Profile\n",
    "\n",
    "Write code in the cell below to plot the eigenvalue profile of the data. This plot helps determine how many principal components to retain for dimensionality reduction. Make sure that all plots are clearly labeled. Each figure must include labeled axes, an appropriate title, and a legend where applicable.\n",
    "\n",
    "- **X-axis:** Eigenvalue indices $(1, 2, \\ldots, 1024)$\n",
    "- **Y-axis:** Eigenvalue magnitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the eigenvalue profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Dimensionality Table\n",
    "\n",
    "Create a table reporting the dimensionality $d$ required to keep 0.9, 0.95, and 0.98 fraction of the total variance. Write code in the cell below to compute these values, then fill in the table.\n",
    "\n",
    "Use the formula:\n",
    "$$d = \\frac{\\sum_{i=1}^{d}\\lambda_i}{\\sum_{i=1}^{n}\\lambda_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate d for variance thresholds: 0.9, 0.95, 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variance Fraction | Dimensionality (d) |\n",
    "|-------------------|-------------------|\n",
    "| 0.90              |                   |\n",
    "| 0.95              |                   |\n",
    "| 0.98              |                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 t-SNE Visualization\n",
    "\n",
    "Visualize the reduced data using t-SNE in a 2-dimensional feature space.\n",
    "\n",
    "- Use different colors for data points from different objects\n",
    "- Every 72 data examples correspond to one object\n",
    "- You can use `sklearn.manifold.TSNE` in Python with the configured perplexity parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize reduced data using t-SNE in 2D\n",
    "# Hint: Use different colors for each of the 20 objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J09p_a_Tgcdh"
   },
   "source": [
    "## 4. Discussion (2 points)\n",
    "\n",
    "Discuss your observations on the obtained results:\n",
    "- What does the eigenvalue profile tell you about the data?\n",
    "- How well does PCA reduce the dimensionality while preserving variance?\n",
    "- What do you observe in the t-SNE visualization? Are the objects well-separated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F__WIApYgs7J"
   },
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on Themis\n",
    "Themis will only grade your implementation of the PCA algorithm, thus giving a maximum of `2.5` points. It does so by executing every cell up to and including the PCA call. Make sure your code runs without errors and produces the expected outputs before submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution\n",
    "\n",
    "State your individual contribution.\n",
    "\n",
    "**Your answer here:**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
